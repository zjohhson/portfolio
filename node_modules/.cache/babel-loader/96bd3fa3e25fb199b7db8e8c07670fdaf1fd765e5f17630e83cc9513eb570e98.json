{"ast":null,"code":"var _jsxFileName = \"/Users/zachjohnson/Desktop/portfolio/src/Pages/Home/Jailbreak.jsx\";\nimport BackHomeNavbar from \"./BackHomeNavBar\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nexport default function Jailbreak() {\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(BackHomeNavbar, {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 6,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 7,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 8,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 9,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"section\", {\n      children: /*#__PURE__*/_jsxDEV(\"div\", {\n        children: [/*#__PURE__*/_jsxDEV(\"div\", {\n          style: {\n            display: \"flex\",\n            justifyContent: \"center\",\n            background: '#E5F3FD',\n            padding: '30px'\n          },\n          children: /*#__PURE__*/_jsxDEV(\"h2\", {\n            children: \"Generative Adversarial Learning for Automatic Red-Teaming against LargeLanguage Model Jailbreak Attacks\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 14,\n            columnNumber: 13\n          }, this)\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 12,\n          columnNumber: 11\n        }, this), /*#__PURE__*/_jsxDEV(\"h3\", {\n          children: [\"See project proposal \", /*#__PURE__*/_jsxDEV(\"a\", {\n            href: \"https://drive.google.com/file/d/1A7fVwS_VQZAYP8dN3wMqcdA_pfkH1IyH/view?usp=sharing\",\n            children: \"here\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 16,\n            columnNumber: 38\n          }, this), \".\"]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 16,\n          columnNumber: 13\n        }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n          style: {\n            textAlign: 'center',\n            background: '#E5F3FD',\n            padding: '30px'\n          },\n          children: [/*#__PURE__*/_jsxDEV(\"p\", {\n            children: \"Jailbreaking\\u2019 Large Language Models (LLMs) \\u2014 feeding in prompts that bypass the model\\u2019s safety guardrails and cause it to produce a harmful output \\u2014 is a critical safety issue impacting the world of Generative AI, and yet is impossible to avoid completely due to its dynamic nature. While popular Generative AI language models such as ChatGPT aim to minimize outputting hate speech, misinformation, violent text, instructions on building weapons, etc., they are not entirely successful. Despite developers implementing as many safeguards as possible to defend against known adversarial attacks, bad actors are constantly devising new and creative ways to jailbreak LLMs. Thus, the pool of jailbreak prompts for popularly used LLMs is always growing.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 18,\n            columnNumber: 13\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 25,\n            columnNumber: 20\n          }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n            children: \"Red-teaming \\u2014 the practice of stress-testing a model via input prompts to discover its weak spots and possible areas of attack \\u2014 is a popular method for identifying and defending against adversarial jailbreak prompts that can jailbreak a LLM. Considering AI\\u2019s rapid development, coupled with its current lack of a structured regulation or auditing framework, red-teaming is the industry standard for guaranteeing safety and robustness of Generative AI models.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 26,\n            columnNumber: 1\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 30,\n            columnNumber: 44\n          }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n            children: \"This approach, however, is largely static in nature. Red-teaming is conventionally done by gathering talented developers to manually brainstorm potential jailbreak prompts, and then test these on LLMs. Prompts that successfully yield a jailbreak attack are stored in some central database, and are used to fine-tune the LLM so as to not fall victim to these prompts in future iterations.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 31,\n            columnNumber: 1\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 34,\n            columnNumber: 86\n          }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n            children: \"There are several pitfalls to this approach. For example, it is unscalable as the number and creativity of jailbreak prompts devised by adversaries increases. According to OpenAI\\u2019s page on its Red-Teaming Network, they conduct a rigorous application process to select a pool of skilled experts in fields such as law, cybersecurity, biology, anthropology, computer science, etc. to robustly and effectively discover weak spots in ChatGPT. This process of carefully selecting applicants for each iteration of GPT, along with manuall probing the model to find its weak points, is an incredibly time and resource intensive process that may be difficult to uphold as more models and iterations of LLMs are developed.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 35,\n            columnNumber: 1\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 41,\n            columnNumber: 77\n          }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n            children: \"For my Master's of Engineering thesis project, I aim to improve the safety of Large Language Generative AI Models by addressing the pitfalls of current red teaming approaches by creating a dynamic and algorithmic framework for generating and detecting unforeseen jailbreak prompts. Specifically, I use a Generative Adversarial Network (GAN) to achieve two outcomes: 1) leveraging an adversary to generate a rich and dynamic dataset of jailbreak prompts that can be used for enhancing the safety of a currently existing LLM, and 2) utilizing these adversarial generations to train a discriminator model that can robustly identify unforeseen jailbreak prompts for LLMs. This approach is both scalable and robust. By algorithmically generating jailbreak prompts as opposed to manually via a team of experts, we can greatly increase the scale and creativity of the jailbreak prompts that we use for red-teaming. This allows us to create a rich dataset, including unconventional and unforeseen jailbreak prompts, which will in turn yield more robust and safe defense against adversarial attacks against LLMs.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 42,\n            columnNumber: 1\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 52,\n            columnNumber: 46\n          }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n            children: \"Currently (as of Fall 2023), this project is ongoing, and as more progress is made, it will be updated here.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 53,\n            columnNumber: 1\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 17,\n          columnNumber: 13\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 11,\n        columnNumber: 9\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 10,\n      columnNumber: 7\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 5,\n    columnNumber: 7\n  }, this);\n}\n_c = Jailbreak;\nvar _c;\n$RefreshReg$(_c, \"Jailbreak\");","map":{"version":3,"names":["BackHomeNavbar","jsxDEV","_jsxDEV","Jailbreak","children","fileName","_jsxFileName","lineNumber","columnNumber","style","display","justifyContent","background","padding","href","textAlign","_c","$RefreshReg$"],"sources":["/Users/zachjohnson/Desktop/portfolio/src/Pages/Home/Jailbreak.jsx"],"sourcesContent":["import BackHomeNavbar from \"./BackHomeNavBar\";\nexport default function Jailbreak() {\n\n    return (\n      <div>\n      <BackHomeNavbar/>\n      <br/>\n      <br/>\n      <br/>\n      <section>\n        <div >\n          <div style={{display: \"flex\", justifyContent: \"center\",background: '#E5F3FD', padding: '30px'}}>\n            {/* <p className=\"section--title\">About</p> */}\n            <h2>Generative Adversarial Learning for Automatic Red-Teaming against LargeLanguage Model Jailbreak Attacks</h2>\n            </div>\n            <h3>See project proposal <a href='https://drive.google.com/file/d/1A7fVwS_VQZAYP8dN3wMqcdA_pfkH1IyH/view?usp=sharing'>here</a>.</h3>\n            <div style={{textAlign: 'center', background: '#E5F3FD', padding: '30px'}}>\n            <p>Jailbreaking’ Large Language Models (LLMs) — feeding in prompts that bypass the model’s safety\nguardrails and cause it to produce a harmful output — is a critical safety issue impacting the world of\nGenerative AI, and yet is impossible to avoid completely due to its dynamic nature. While popular Generative\nAI language models such as ChatGPT aim to minimize outputting hate speech, misinformation, violent text,\ninstructions on building weapons, etc., they are not entirely successful. Despite developers implementing as\nmany safeguards as possible to defend against known adversarial attacks, bad actors are constantly devising\nnew and creative ways to jailbreak LLMs. Thus, the pool of jailbreak prompts for popularly used LLMs is\nalways growing.</p><br/>\n<p>Red-teaming — the practice of stress-testing a model via input prompts to discover its weak spots and\npossible areas of attack — is a popular method for identifying and defending against adversarial jailbreak\nprompts that can jailbreak a LLM. Considering AI’s rapid development, coupled with its current lack of a\nstructured regulation or auditing framework, red-teaming is the industry standard for guaranteeing safety\nand robustness of Generative AI models.</p><br/>\n<p>This approach, however, is largely static in nature. Red-teaming is conventionally done by gathering\ntalented developers to manually brainstorm potential jailbreak prompts, and then test these on LLMs.\nPrompts that successfully yield a jailbreak attack are stored in some central database, and are used to\nfine-tune the LLM so as to not fall victim to these prompts in future iterations.</p><br/>\n<p>There are several pitfalls to this approach. For example, it is unscalable as the number and creativity\nof jailbreak prompts devised by adversaries increases. According to OpenAI’s page on its Red-Teaming\nNetwork, they conduct a rigorous application process to select a pool of skilled experts in fields such as law,\ncybersecurity, biology, anthropology, computer science, etc. to robustly and effectively discover weak spots\nin ChatGPT. This process of carefully selecting applicants for each iteration of GPT, along with manuall\nprobing the model to find its weak points, is an incredibly time and resource intensive process that may be\ndifficult to uphold as more models and iterations of LLMs are developed.</p><br/>\n<p>For my Master's of Engineering thesis project, I aim to improve the safety of Large Language Generative AI Models by addressing the\n pitfalls of current red teaming approaches by creating a dynamic and algorithmic framework\nfor generating and detecting unforeseen jailbreak prompts. Specifically, I use a Generative Adversarial\nNetwork (GAN) to achieve two outcomes: 1) leveraging an adversary to generate a rich and dynamic dataset\nof jailbreak prompts that can be used for enhancing the safety of a currently existing LLM, and 2) utilizing\nthese adversarial generations to train a discriminator model that can robustly identify unforeseen jailbreak\nprompts for LLMs. This approach is both scalable and robust. By algorithmically generating jailbreak\nprompts as opposed to manually via a team of experts, we can greatly increase the scale and creativity\nof the jailbreak prompts that we use for red-teaming. This allows us to create a rich dataset, including\nunconventional and unforeseen jailbreak prompts, which will in turn yield more robust and safe defense\nagainst adversarial attacks against LLMs.</p><br/>\n<p>Currently (as of Fall 2023), this project is ongoing, and as more progress is made, it will be updated here.</p>\n          </div>\n          {/* <div style={{display: 'flex', justifyContent: 'center'}}>\n          <a href='https://chatnyt-1acc81fc0a7a.herokuapp.com/' target='_blank'>\n                <div>\n            <button  style={{display: 'flex', justifyContent: 'center', alignContent: 'center'}}className='btn btn-outline-primary'>Play ChatNYT!</button></div></a>\n        </div> */}\n        </div>\n      </section>\n      </div>\n    );\n  }\n  "],"mappings":";AAAA,OAAOA,cAAc,MAAM,kBAAkB;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAC9C,eAAe,SAASC,SAASA,CAAA,EAAG;EAEhC,oBACED,OAAA;IAAAE,QAAA,gBACAF,OAAA,CAACF,cAAc;MAAAK,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAC,CAAC,eACjBN,OAAA;MAAAG,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAI,CAAC,eACLN,OAAA;MAAAG,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAI,CAAC,eACLN,OAAA;MAAAG,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAI,CAAC,eACLN,OAAA;MAAAE,QAAA,eACEF,OAAA;QAAAE,QAAA,gBACEF,OAAA;UAAKO,KAAK,EAAE;YAACC,OAAO,EAAE,MAAM;YAAEC,cAAc,EAAE,QAAQ;YAACC,UAAU,EAAE,SAAS;YAAEC,OAAO,EAAE;UAAM,CAAE;UAAAT,QAAA,eAE7FF,OAAA;YAAAE,QAAA,EAAI;UAAuG;YAAAC,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAI;QAAC;UAAAH,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAC3G,CAAC,eACNN,OAAA;UAAAE,QAAA,GAAI,uBAAqB,eAAAF,OAAA;YAAGY,IAAI,EAAC,oFAAoF;YAAAV,QAAA,EAAC;UAAI;YAAAC,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAG,CAAC,KAAC;QAAA;UAAAH,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OAAI,CAAC,eACpIN,OAAA;UAAKO,KAAK,EAAE;YAACM,SAAS,EAAE,QAAQ;YAAEH,UAAU,EAAE,SAAS;YAAEC,OAAO,EAAE;UAAM,CAAE;UAAAT,QAAA,gBAC1EF,OAAA;YAAAE,QAAA,EAAG;UAOA;YAAAC,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAG,CAAC,eAAAN,OAAA;YAAAG,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAI,CAAC,eACxBN,OAAA;YAAAE,QAAA,EAAG;UAIoC;YAAAC,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAG,CAAC,eAAAN,OAAA;YAAAG,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAI,CAAC,eAChDN,OAAA;YAAAE,QAAA,EAAG;UAG8E;YAAAC,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAG,CAAC,eAAAN,OAAA;YAAAG,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAI,CAAC,eAC1FN,OAAA;YAAAE,QAAA,EAAG;UAMqE;YAAAC,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAG,CAAC,eAAAN,OAAA;YAAAG,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAI,CAAC,eACjFN,OAAA;YAAAE,QAAA,EAAG;UAUsC;YAAAC,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAG,CAAC,eAAAN,OAAA;YAAAG,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAI,CAAC,eAClDN,OAAA;YAAAE,QAAA,EAAG;UAA4G;YAAAC,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAAG,CAAC;QAAA;UAAAH,QAAA,EAAAC,YAAA;UAAAC,UAAA;UAAAC,YAAA;QAAA,OACpG,CAAC;MAAA;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAMH;IAAC;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACC,CAAC;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACL,CAAC;AAEV;AAACQ,EAAA,GA9DqBb,SAAS;AAAA,IAAAa,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}